{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Авторы материала: Юрий Кашницкий (@yorko) и Максим Уваров (@maxis42). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Домашнее задание № 3 \n",
    "## <center> Деревья решений для классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В этом задании мы разберемся с тем, как работает дерево решений в задаче регрессии, а также построим (и настроим) классифицирующие деревья решений в задаче прогнозирования сердечно-сосудистых заболеваний. \n",
    "Заполните код в клетках (где написано \"Ваш код здесь\") и ответьте на вопросы в [веб-форме](https://docs.google.com/forms/d/1bXgaJRh4naJOGzE_Li2k50TN0kOPJdNhMtHl6H4t3Lk/edit).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Простой пример восстановления регрессии с помощью дерева решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующую одномерную задачу восстановления регрессии. Неформально, надо построить функцию $a(x)$, приближающую искомую зависимость $y = f(x)$ в терминах среднеквадратичной ошибки: $min \\sum_i {(a(x_i) - f(x_i))}^2$. Подробно мы рассмотрим эту задачу в следующий раз ([4-я статья курса](https://habrahabr.ru/company/ods/blog/323890/)), а пока поговорим о том, как решать эту задачу с помощью дерева решений. Предварительно прочитайте небольшой раздел [\"Дерево решений в задаче регрессии\"](https://habrahabr.ru/company/ods/blog/322534/#derevo-resheniy-v-zadache-regressii) 3-ей статьи курса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-2, 2, 7)\n",
    "y = X ** 3\n",
    "data = np.c_[X, y]\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделаем несколько шагов в построении дерева решений. Исходя из соображений симметрии, выберем пороги для разбиения равными соответственно 0, 1.5 и -1.5. Напомним, что в случае задачи восстановления регрессии листовая вершина выдает среднее значение ответа по всем объектам обучающей выборки, попавшим в эту вершину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, начнём. Дерево глубины 0 состоит из одного корня, который содержит всю обучающую выборку. Как будут выглядеть предсказания данного дерева для $x \\in [-2, 2]$? Постройте соответствующий график. Тут без `sklearn` – разбираемся просто с ручкой, бумажкой и Python, если надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean(x):\n",
    "    size = len(x)\n",
    "    return [sum(x) / size]*size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, get_mean(y))\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем первое разбиение выборки по предикату $[x < 0]$. Получим дерево глубины 1 с двумя листьями. Постройте аналогичный график предсказаний для этого дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "p1 = y[X < 0].mean()\n",
    "p2 = y[X >= 0].mean()\n",
    "\n",
    "plt.plot([-2, 0-eps, 0+eps, 2], [p1, p1,p2, p2])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме построения дерева решений признак и значение порога, по которым происходит разбиение выборки, выбираются исходя из некоторого критерия. Для регрессии обычно используется дисперсионный критерий:\n",
    "$$Q(X, j, t) = D(X) - \\dfrac{|X_l|}{|X|} D(X_l) - \\dfrac{|X_r|}{|X|} D(X_r),$$\n",
    "где $X$ – выборка, находящаяся в текущей вершине, $X_l$ и $X_r$ – разбиение выборки $X$ на две части по предикату $[x_j < t]$ (то есть по $j$-ому признаку и порогу $t$), $|X|$, $|X_l|$, $|X_r|$ - размеры соответствующих выборок, а $D(X)$ – дисперсия ответов на выборке $X$:\n",
    "$$D(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}(y_j – \\dfrac{1}{|X|}\\sum_{x_i \\in X}y_i)^2,$$\n",
    "где $y_i = y(x_i)$ – ответ на объекте $x_i$. При каждом разбиении вершины выбираются признак $j$ и значение порога $t$, максимизирующие значение функционала $Q(X, j, t)$.\n",
    "\n",
    "В нашем случае признак всего один, поэтому $Q$ зависит только от значения порога $t$ (и ответов выборки в данной вершине). \n",
    "\n",
    "Постройте график функции $Q(X, t)$ в корне в зависимости от значения порога $t$ на отрезке $[-1.9, 1.9]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_var_criterion(X, Y, t):\n",
    "    def D(x, y):\n",
    "        return ((y - y.mean())**2).mean()\n",
    "    Xl, Yl = X[X < t], Y[X < t]\n",
    "    Xr, Yr = X[X >= t], Y[X >= t]\n",
    "    return D(X, Y) - Xl.size / X.size * D(Xl, Yl) - Xr.size / X.size * D(Xr, Yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(-1.9, 1.9, 20)\n",
    "plt.plot(T, [regression_var_criterion(X, y, t) for t in T])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 1.</font> Оптимально ли с точки зрения дисперсионного критерия выбранное нами значение порога $t = 0$?**\n",
    "- **Да**\n",
    "- Нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь произведем разбиение в каждой из листовых вершин. В левой (соответствующей ветви $x < 0$) – по предикату $[x < -1.5]$, а в правой (соответствующей ветви $x \\geqslant 0$) – по предикату $[x < 1.5]$. Получится дерево глубины 2 с 7 вершинами и 4 листьями. Постройте график предсказаний этого дерева для $x \\in [-2, 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [0, -1.5, 1.5]\n",
    "eps = 0.0001\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "root_y_l, root_x_l = y[X < points[0]], X[X < points[0]]\n",
    "root_y_r, root_x_r = y[X >= points[0]], X[X >= points[0]]\n",
    "print(root_l)\n",
    "p1 = root_l[root_x_l < points[1]].mean()\n",
    "p2 = root_l[root_x_l >= points[1]].mean()\n",
    "p3 = root_r[root_x_r < points[2]].mean()\n",
    "p4 = root_r[root_x_r >= points[2]].mean()\n",
    "\n",
    "\n",
    "plt.plot([-2, -1.5-eps, -1.5+eps, 0-eps, 0+eps, 1.5-eps, 1.5+eps, 2], [p1, p1,p2, p2, p3, p3, p4, p4 ])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$');\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 2.</font> Из какого числа отрезков состоит график (необходимо считать как горизонтальные, так и вертикальные прямые), изображающий предсказания построенного дерева на отрезке [-2, 2]?**\n",
    "- 5\n",
    "- 6\n",
    "- 7\n",
    "- 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Построение дерева решений для прогноза сердечно-сосудистых заболеваний\n",
    "Считаем в `DataFrame` знакомый нам набор данных по сердечно-сосудистым заболеваниям. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', \n",
    "                 index_col='id', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте небольшие преобразования признаков: постройте признак \"возраст в годах\" (полных лет), а также постройте по 3 бинарных признака на основе `cholesterol` и `gluc`, где они, соответственно, равны 1, 2 или 3. Эта техника называется dummy-кодированием или One Hot Encoding (OHE), удобней всего в данном случае использовать `pandas.get_dummmies`. Исходные признаки `cholesterol` и `gluc` после кодирования использовать не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_years'] = (df['age'] // 365.25).astype(int)\n",
    "cholesterol_dummies = pd.get_dummies(df.cholesterol, prefix=\"cholesterol\")\n",
    "gluc_dummies = pd.get_dummies(df.gluc, prefix=\"gluc\")\n",
    "df = pd.concat([df, cholesterol_dummies, gluc_dummies], axis=1)\n",
    "y = df['cardio']\n",
    "df.drop(['cholesterol', 'gluc', 'cardio'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте выборку на обучающую и отложенную (holdout) части в пропорции 7/3. Для этого используйте метод `sklearn.model_selection.train_test_split`, зафиксируйте у него `random_state`=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df.values, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите на выборке `(X_train, y_train)` дерево решений с ограничением на максимальную глубину в 3. Зафиксируйте у дерева `random_state=17`. Визуализируйте дерево с помошью `sklearn.tree.export_graphviz`, `dot` и `pydot`. Пример дан в [статье](https://habrahabr.ru/company/ods/blog/322534/) под спойлером \"Код для отрисовки дерева\". Названия файлов писать без кавычек, для того чтобы работало в jupyter notebook. Обратите внимание, что команды в Jupyter notebook, начинающиеся с восклицательного знака – это терминальные команды (которые мы обычно запускаем в терминале/командной строке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=17)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "         \n",
    "export_graphviz(tree, feature_names=list(df.columns.values), out_file='C:/Users/nikit/Desktop/usml2018/week_3/des_tree.dot', filled=True)\n",
    "!dot -Tps 'C:/Users/nikit/Desktop/usml2018/week_3/des_tree.dot' -o 'C:/Users/nikit/Desktop/usml2018/week_3/des_tree.ps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 3.</font> Какие 3 признака задействуются при прогнозе в построенном дереве решений? (то есть эти три признака \"можно найти в дереве\")**\n",
    "- weight, height, gluc=3\n",
    "- smoke, age, gluc=3\n",
    "- age, weight, chol=3\n",
    "- age, ap_hi, chol=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте с помощью обученного дерева прогноз для отложенной выборки `(X_valid, y_valid)`. Посчитайте долю верных ответов (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pred = tree.predict(X_valid)\n",
    "acc1 = accuracy_score(y_valid, tree_pred)\n",
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь на кросс-валидации по выборке  `(X_train, y_train)`  настройте глубину дерева, чтобы повысить качество модели. Используйте GridSearchCV, 5-кратную кросс-валидацию. Зафиксируйте у дерева `random_state`=17. Перебирайте параметр `max_depth` от 2 до 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'max_depth': list(range(2, 11))}\n",
    "\n",
    "tree_grid = GridSearchCV(tree, tree_params, cv=5, n_jobs=-1, verbose=True)\n",
    "tree_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте график того, как меняется средняя доля верных ответов на кросс-валидации в зависимости от значения `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_valid = np.array([[x[0]['max_depth'], x[1]] for x in tree_grid.grid_scores_])\n",
    "plt.plot(cross_valid[:, 0], cross_valid[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите лучшее значение `max_depth`, то есть такое, при котором среднее значение метрики качества на кросс-валидации максимально. Посчитайте также, какова теперь доля верных ответов на отложенной выборке. Все это можно сделать с помощью обученного экземпляра класса `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_depth = tree_grid.best_params_['max_depth']\n",
    "acc2 = accuracy_score(y_valid, tree_grid.predict(X_valid))\n",
    "print(\"Best depth:{}\\nAccuracy:{}\".format(best_max_depth, acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 4.</font> Имеется ли на кривой валидации по максимальной глубине дерева пик `accuracy`, если перебирать `max_depth` от 2 до 10? Повысила ли настройка глубины дерева качество классификации (accuracy) более чем на 1% на отложенной выборке (надо посмотреть на выражение (acc2 - acc1) / acc1 * 100%, где acc1 и acc2 – доли верных ответов на отложенной выборке до и после настройки max_depth соответственно)?**\n",
    "- да, да\n",
    "- да, нет\n",
    "- нет, да\n",
    "- нет, нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acc2 - acc1) / acc1 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся опять (как и в 1 домашке) к картинке, демонстрирующей шкалу SCORE для расчёта риска смерти от сердечно-сосудистого заболевания в ближайшие 10 лет.\n",
    "<img src='../../img/SCORE2007.png' width=70%>\n",
    "\n",
    "Создайте бинарные признаки, примерно соответствующие этой картинке:\n",
    "- $age \\in [45,50), \\ldots age \\in [60,65) $ (4 признака)\n",
    "- верхнее артериальное давление: $ap\\_hi \\in [120,140), ap\\_hi \\in [140,160), ap\\_hi \\in [160,180),$ (3 признака)\n",
    "\n",
    "Если значение возраста или артериального давления не попадает ни в один из интервалов, то все бинарные признаки будут равны нулю. Далее будем строить дерево решений с этим признаками, а также с признаками ``smoke``, ``cholesterol``  и ``gender``. Из признака ``cholesterol`` надо сделать 3 бинарных, соотв-х уникальным значениям признака ( ``cholesterol``=1,  ``cholesterol``=2 и  ``cholesterol``=3), эта техника называется dummy-кодированием или One Hot Encoding (OHE). Признак ``gender`` надо перекодировать: значения 1 и 2 отобразить на 0 и 1. Признак лучше переименовать в ``male`` (0 – женщина, 1 – мужчина). В общем случае кодирование значений делает ``sklearn.preprocessing.LabelEncoder``, но в данном случае легко обойтись и без него.\n",
    "\n",
    "Итак, дерево решений строится на 12 бинарных признаках (исходные признаки не берем).\n",
    "\n",
    "Постройте дерево решений с ограничением на максимальную глубину = 3 и обучите его на всей исходной обучающей выборке. Используйте `DecisionTreeClassifier`, на всякий случай зафикисровав `random_state=17`, остальные аргументы (помимо `max_depth` и `random_state`) оставьте по умолчанию. \n",
    "\n",
    "**<font color='red'>Вопрос 5.</font> Какой бинарный признак из 12 перечисленных оказался самым важным для обнаружения ССЗ, то есть поместился в вершину построенного дерева решений?**\n",
    "- Верхнее артериальное давление от 160 до 180 (мм рт.ст.)\n",
    "- Пол мужской / женский\n",
    "- Верхнее артериальное давление от 140 до 160 (мм рт.ст.)\n",
    "- Возраст от 50 до 55 (лет)\n",
    "- Курит / не курит\n",
    "- Возраст от 60 до 65 (лет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def is_between(x, a, b):\n",
    "    return int(a <= x < b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = df[['smoke', 'cholesterol_1', 'cholesterol_2', 'cholesterol_3']]\n",
    "\n",
    "score_df['age_45'] = df['age_years'].apply(lambda x: int(45 <= x < 50))\n",
    "score_df['age_50'] = df['age_years'].apply(lambda x: int(50 <= x < 55))\n",
    "score_df['age_55'] = df['age_years'].apply(lambda x: int(55 <= x < 60))\n",
    "score_df['age_60'] = df['age_years'].apply(lambda x: int(60 <= x < 65))\n",
    "\n",
    "score_df['ap_hi_120'] = df['ap_hi'].apply(lambda x: int(120 <= x < 140))\n",
    "score_df['ap_hi_140'] = df['ap_hi'].apply(lambda x: int(140 <= x < 160))\n",
    "score_df['ap_hi_160'] = df['ap_hi'].apply(lambda x: int(160 <= x < 180))\n",
    "score_df['male'] = df['gender'] - 1\n",
    "score_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=17)\n",
    "tree.fit(score_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree, feature_names=list(score_df.columns.values),\n",
    "out_file='C:/Users/nikit/Desktop/usml2018/week_3/tree.dot', filled=True)\n",
    "!dot -Tps 'C:/Users/nikit/Desktop/usml2018/week_3/tree.dot' -o 'C:/Users/nikit/Desktop/usml2018/week_3/tree.ps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "lesson4_part2_Decision_trees.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
