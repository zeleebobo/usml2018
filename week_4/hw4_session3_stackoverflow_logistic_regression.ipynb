{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Автор материала: Павел Нестеров (@mephistopheies). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.14.0\n",
      "scipy 1.0.0\n",
      "pandas 0.22.0\n",
      "matplotlib 2.1.2\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 5.4.0 20160609\n",
      "system     : Linux\n",
      "release    : 4.9.60-linuxkit-aufs\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 2\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'java', 'c++', 'html', 'python', 'jquery', 'android', 'javascript', 'c#', 'php', 'ios'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    #top_tags = list(map(str.strip, f))\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$ *\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font>В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$ *\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #z = (-10, z, 10)[int(z > -10) + int(z > 10)]\n",
    "                    #sigma = 1 / (1 + np.exp(-z))\n",
    "                    #sigma = 1 / (1 + np.exp(-z)) if z >= 0 else 1 - (1 / (1 + np.exp(z)))\n",
    "                   \n",
    "                    sigma = .5 + .5 * np.tanh(z / 2)\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss -= y * np.log(max([sigma, tolerance])) + (1 - y) * np.log(max([1 - sigma, tolerance]))   \n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726154706b344310b52761c65eafb2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43a204c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.67\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = []\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #z = (-10, z, 10)[int(z > -10) + int(z > 10)]\n",
    "                    #sigma = 1 / (1 + np.exp(-z))\n",
    "                    #sigma = 1 / (1 + np.exp(-z)) if z >= 0 else 1 - (1 / (1 + np.exp(z)))\n",
    "                   \n",
    "                    sigma = .5 + .5 * np.tanh(z / 2)\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss -= y * np.log(max([sigma, tolerance])) + (1 - y) * np.log(max([1 - sigma, tolerance]))   \n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    else:\n",
    "                        if sigma > 0.9:\n",
    "                            pred_tags.add(tag)\n",
    "                    \n",
    "                if n >= top_n_train:\n",
    "                    acc.append(len(pred_tags & tags) / len(pred_tags | tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df41e48312c4457bdbace59f956f8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
